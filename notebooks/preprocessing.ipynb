{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4625406",
   "metadata": {},
   "source": [
    "# Getting the most solar power for your dollar\n",
    "## Preprocessing and feature engineering\n",
    "### Zachary Brown\n",
    "\n",
    "The data has been cleaned and preliminary analysis has identified some trends we should expect to see the eventual model pick up on. Now I'm going to preprocess the data so that any models I work with can use the data appropriately. This will include imputing missing data, feature engineering, scaling, and splitting the data into testing and training datasets.\n",
    "\n",
    "I'll start by loading the necessary packages and reading in the data from the exploratory data analysis portion of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c8ce0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme('notebook')\n",
    "import scipy.stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, f_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6575f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zachary Brown\\Documents\\GitHub\\Solar-Panel-Capstone\\notebooks\n",
      "C:\\Users\\Zachary Brown\\Documents\\GitHub\\Solar-Panel-Capstone\\data\\processed\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "os.chdir(r\"..\\data\\processed\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49c67a1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208257, 57)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('processed_data.csv', index_col=0, na_values = [-1, '-1'], low_memory=False)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2648a756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{int64: ['expansion_system', 'multiple_phase_system', 'year', 'month', 'state_AZ', 'state_CA', 'state_CO', 'state_CT', 'state_DE', 'state_FL', 'state_MA', 'state_MD', 'state_MN', 'state_NH', 'state_NM', 'state_NY', 'state_RI', 'state_TX', 'state_UT', 'state_WI'], float64: ['system_size_dc', 'total_installed_price', 'rebate_or_grant', 'tracking', 'ground_mounted', 'third_party_owned', 'self_installed', 'azimuth_1', 'tilt_1', 'module_quantity_1', 'additional_modules', 'bipv_module_1', 'bifacial_module_1', 'nameplate_capacity_module_1', 'efficiency_module_1', 'inverter_quantity_1', 'additional_inverters', 'micro_inverter_1', 'solar_storage_hybrid_inverter_1', 'built_in_meter_inverter_1', 'dc_optimizer', 'inverter_loading_ratio', 'price_per_kw'], object: ['data_provider_1', 'system_id_1', 'installation_date', 'customer_segment', 'zip_code', 'city', 'utility_service_territory', 'installer_name', 'module_manufacturer_1', 'module_model_1', 'technology_module_1', 'inverter_manufacturer_1', 'inverter_model_1', 'date_of_battery_install']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns.groupby(data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b5a9b5",
   "metadata": {},
   "source": [
    "I'm going to first remove total installed price because it's not something that's really in the user's control. They may be able to shop around for a few quotes, but in reality the goal of the project is to determine what other levers can be pulled to reduce that price as much as possible. Next I'll remove city since the goal of my project is just to analyze cost efficiency at the state level. Zip code may give some interesting insights on a more granular level, but I don't see additional value in keeping city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a360a78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop(columns=['total_installed_price', 'city'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd3bcf3",
   "metadata": {},
   "source": [
    "I want to check each column for the percentage of values that are missing, and remove any features with more than 30% missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "899b66fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date_of_battery_install            93.650634\n",
       "ground_mounted                     27.698949\n",
       "azimuth_1                          22.536097\n",
       "tilt_1                             22.512569\n",
       "tracking                           21.979573\n",
       "inverter_loading_ratio             21.270353\n",
       "additional_modules                 18.227479\n",
       "additional_inverters               18.227479\n",
       "solar_storage_hybrid_inverter_1    17.041444\n",
       "inverter_quantity_1                14.037463\n",
       "module_quantity_1                  14.007212\n",
       "efficiency_module_1                12.028887\n",
       "built_in_meter_inverter_1          11.630341\n",
       "micro_inverter_1                   11.630341\n",
       "inverter_manufacturer_1            11.622178\n",
       "inverter_model_1                   11.621698\n",
       "bipv_module_1                      11.000351\n",
       "dc_optimizer                       10.992668\n",
       "nameplate_capacity_module_1        10.799637\n",
       "technology_module_1                10.606126\n",
       "bifacial_module_1                  10.606126\n",
       "module_model_1                     10.605646\n",
       "module_manufacturer_1              10.605646\n",
       "third_party_owned                   7.241053\n",
       "self_installed                      6.121283\n",
       "zip_code                            5.934494\n",
       "system_id_1                         3.869738\n",
       "utility_service_territory           2.913708\n",
       "installer_name                      2.110373\n",
       "state_CT                            0.000000\n",
       "state_NH                            0.000000\n",
       "state_NM                            0.000000\n",
       "state_NY                            0.000000\n",
       "state_RI                            0.000000\n",
       "state_TX                            0.000000\n",
       "state_UT                            0.000000\n",
       "state_MN                            0.000000\n",
       "state_MD                            0.000000\n",
       "state_MA                            0.000000\n",
       "state_FL                            0.000000\n",
       "state_DE                            0.000000\n",
       "data_provider_1                     0.000000\n",
       "state_CO                            0.000000\n",
       "state_CA                            0.000000\n",
       "state_AZ                            0.000000\n",
       "price_per_kw                        0.000000\n",
       "month                               0.000000\n",
       "year                                0.000000\n",
       "multiple_phase_system               0.000000\n",
       "expansion_system                    0.000000\n",
       "customer_segment                    0.000000\n",
       "rebate_or_grant                     0.000000\n",
       "system_size_dc                      0.000000\n",
       "installation_date                   0.000000\n",
       "state_WI                            0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percent_missing = data.isnull().sum()/len(data)*100\n",
    "percent_missing.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62deb0c",
   "metadata": {},
   "source": [
    "The only feature that needs to be removed due to null values is 'date_of_battery_install'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3d26036",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop(columns=['date_of_battery_install'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e571de98",
   "metadata": {},
   "source": [
    "I'm going to check how many zip codes are in the dataset, convert them from 9 digit to 5, and then check to see how many times the top five unique zip codes are present in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "672707e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7602"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['zip_code'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "386297a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['zip_code'] = data['zip_code'].str[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efc0df7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4113\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "92584    1141\n",
       "92058    1128\n",
       "92336    1047\n",
       "93727     982\n",
       "95762     908\n",
       "Name: zip_code, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data['zip_code'].nunique())\n",
    "data['zip_code'].value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04a525a",
   "metadata": {},
   "source": [
    "So it looks like removing the last four digits from the zip code reduced some of the variance, but even so none of the zip codes are used very much relative to the 200,000 entries the dataset has. I'll check how many zip codes have more than 30 entries and then decide on a threshold for which should get dummy columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92f5eb8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1287\n"
     ]
    }
   ],
   "source": [
    "print((data['zip_code'].value_counts() > 30).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013b6e86",
   "metadata": {},
   "source": [
    "Right now I have 57 features in my data. A general rule of thumb is to keep the number of features limited to the square root of the number of entries in the data. For this dataset that means I should stick to 456 or fewer features. With that said, I will go ahead and create dummy columns for each of these zip codes and then later on I'll use univariate feature selection to retain only the 400 most statistically significant features when subjected to an independent t-test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f976aa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208257, 54)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "032e0f9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208257, 1342)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['zip_other'] = 0\n",
    "small_zips = []\n",
    "\n",
    "for zipcode in data['zip_code'].unique():\n",
    "    if data['zip_code'].value_counts(dropna=False)[zipcode] > 30:\n",
    "        new_col = pd.Series(((data['zip_code'] == zipcode)*1), name = f'zip_{zipcode}')\n",
    "        data = pd.concat([data, new_col.to_frame()], axis=1)\n",
    "    else:\n",
    "        small_zips.append(zipcode)\n",
    "        \n",
    "data.loc[data['zip_code'].isin(small_zips), 'zip_other'] = 1 \n",
    "data = data.drop(columns=['zip_code'])\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b12619",
   "metadata": {},
   "source": [
    "Next I want to browse the object columns and count how many unique values each has. If a feature has too many or only one unique value they won't help identify any trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c105447b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_provider_1  :  22\n",
      "system_id_1  :  200022\n",
      "installation_date  :  530\n",
      "customer_segment  :  1\n",
      "utility_service_territory  :  71\n",
      "installer_name  :  2640\n",
      "module_manufacturer_1  :  156\n",
      "module_model_1  :  2459\n",
      "technology_module_1  :  6\n",
      "inverter_manufacturer_1  :  63\n",
      "inverter_model_1  :  628\n"
     ]
    }
   ],
   "source": [
    "for col in data.columns:\n",
    "    if data[col].dtypes == 'object':\n",
    "        print(col, ' : ', data[col].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a553b3",
   "metadata": {},
   "source": [
    "Based on these results it should be safe to remove system_id_1, as that has a unique value for almost every entry. I'll also drop customer_segment since earlier in the project I limited the dataset to only residential installations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac099dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['system_id_1', 'customer_segment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9cb975",
   "metadata": {},
   "source": [
    "Great! Now I need to encode these categorical features as I did with the zip codes earlier. I'll count the unique values for each column and if a value appears in more than 30 entries then I'll dummy encode it, anything below 30 will get lumped into an 'other' column. Since technology_module_1 and data_provider_1 have fewer than 30 unique values I'll just dummy encode those entire columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7cae4b62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208257, 1366)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.get_dummies(data, columns=['technology_module_1', 'data_provider_1'])\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e97ca9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zachary Brown\\AppData\\Local\\Temp\\ipykernel_17368\\2181767294.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{col}_other'] = 0\n",
      "C:\\Users\\Zachary Brown\\AppData\\Local\\Temp\\ipykernel_17368\\2181767294.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{col}_other'] = 0\n",
      "C:\\Users\\Zachary Brown\\AppData\\Local\\Temp\\ipykernel_17368\\2181767294.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{col}_other'] = 0\n",
      "C:\\Users\\Zachary Brown\\AppData\\Local\\Temp\\ipykernel_17368\\2181767294.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{col}_other'] = 0\n",
      "C:\\Users\\Zachary Brown\\AppData\\Local\\Temp\\ipykernel_17368\\2181767294.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{col}_other'] = 0\n",
      "C:\\Users\\Zachary Brown\\AppData\\Local\\Temp\\ipykernel_17368\\2181767294.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{col}_other'] = 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(208257, 2908)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ['installation_date', 'utility_service_territory', 'installer_name', 'module_manufacturer_1',\\\n",
    "        'module_model_1', 'inverter_manufacturer_1', 'inverter_model_1']\n",
    "for col in cols:\n",
    "    data[f'{col}_other'] = 0\n",
    "    small_vals = []\n",
    "\n",
    "    for val in data[col].unique():\n",
    "        if data[col].value_counts(dropna=False)[val] > 30:\n",
    "            new_col = pd.Series(((data[col] == val)*1), name = f'{col}_{val}')\n",
    "            data = pd.concat([data, new_col.to_frame()], axis=1)\n",
    "        else:\n",
    "            small_vals.append(val)\n",
    "        \n",
    "    data.loc[data[col].isin(small_vals), f'{col}_other'] = 1 \n",
    "    data = data.drop(columns=[col])\n",
    "dummied = data.copy()\n",
    "dummied.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7fbaca19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108019    3036.156250\n",
       "108020    2586.400000\n",
       "108142    4015.238095\n",
       "108175    2788.732394\n",
       "108233    3500.000000\n",
       "Name: price_per_kw, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummied['price_per_kw'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6af6e3",
   "metadata": {},
   "source": [
    "Now that I have dummy columns for all categorical values with 30 or more entries I'm going to perform the train test split so that I make sure I'm imputing, scaling, and performing feature selection entirely based on the training data and not getting data leakage built into my model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "242ba230",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dummied.drop(columns=['price_per_kw'])\n",
    "y = dummied['price_per_kw']\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a734aa78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(156192, 2907) (52065, 2907) (156192,) (52065,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7388f9c",
   "metadata": {},
   "source": [
    "I'm going to impute any missing values with the most common value for that feature and later on once I've done initial modeling I can take a closer look at the most important features to consider whether the imputed values should be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e81a5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zachary Brown\\anaconda3\\lib\\site-packages\\sklearn\\impute\\_base.py:49: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode = stats.mode(array)\n",
      "C:\\Users\\Zachary Brown\\anaconda3\\lib\\site-packages\\sklearn\\impute\\_base.py:49: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode = stats.mode(array)\n",
      "C:\\Users\\Zachary Brown\\anaconda3\\lib\\site-packages\\sklearn\\impute\\_base.py:49: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode = stats.mode(array)\n",
      "C:\\Users\\Zachary Brown\\anaconda3\\lib\\site-packages\\sklearn\\impute\\_base.py:49: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode = stats.mode(array)\n",
      "C:\\Users\\Zachary Brown\\anaconda3\\lib\\site-packages\\sklearn\\impute\\_base.py:49: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode = stats.mode(array)\n",
      "C:\\Users\\Zachary Brown\\anaconda3\\lib\\site-packages\\sklearn\\impute\\_base.py:49: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode = stats.mode(array)\n",
      "C:\\Users\\Zachary Brown\\anaconda3\\lib\\site-packages\\sklearn\\impute\\_base.py:49: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode = stats.mode(array)\n",
      "C:\\Users\\Zachary Brown\\anaconda3\\lib\\site-packages\\sklearn\\impute\\_base.py:49: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode = stats.mode(array)\n",
      "C:\\Users\\Zachary Brown\\anaconda3\\lib\\site-packages\\sklearn\\impute\\_base.py:49: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode = stats.mode(array)\n",
      "C:\\Users\\Zachary Brown\\anaconda3\\lib\\site-packages\\sklearn\\impute\\_base.py:49: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode = stats.mode(array)\n",
      "C:\\Users\\Zachary Brown\\anaconda3\\lib\\site-packages\\sklearn\\impute\\_base.py:49: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode = stats.mode(array)\n",
      "C:\\Users\\Zachary Brown\\anaconda3\\lib\\site-packages\\sklearn\\impute\\_base.py:49: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode = stats.mode(array)\n",
      "C:\\Users\\Zachary Brown\\anaconda3\\lib\\site-packages\\sklearn\\impute\\_base.py:49: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode = stats.mode(array)\n",
      "C:\\Users\\Zachary Brown\\anaconda3\\lib\\site-packages\\sklearn\\impute\\_base.py:49: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode = stats.mode(array)\n",
      "C:\\Users\\Zachary Brown\\anaconda3\\lib\\site-packages\\sklearn\\impute\\_base.py:49: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode = stats.mode(array)\n",
      "C:\\Users\\Zachary Brown\\anaconda3\\lib\\site-packages\\sklearn\\impute\\_base.py:49: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode = stats.mode(array)\n",
      "C:\\Users\\Zachary Brown\\anaconda3\\lib\\site-packages\\sklearn\\impute\\_base.py:49: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode = stats.mode(array)\n",
      "C:\\Users\\Zachary Brown\\anaconda3\\lib\\site-packages\\sklearn\\impute\\_base.py:49: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode = stats.mode(array)\n",
      "C:\\Users\\Zachary Brown\\anaconda3\\lib\\site-packages\\sklearn\\impute\\_base.py:49: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode = stats.mode(array)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "mode_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "for col in X_train.columns:\n",
    "    if X_train[col].isna().sum() != 0:\n",
    "        X_train[col] = mode_imputer.fit_transform(X_train[col].values.reshape(-1,1))\n",
    "        X_test[col] = mode_imputer.transform(X_test[col].values.reshape(-1,1))\n",
    "        \n",
    "X_train_imp = X_train.copy()\n",
    "X_test_imp = X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a380255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(156192, 2907)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_imp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9e882c",
   "metadata": {},
   "source": [
    "Now that the data is dummied, split, and imputed, I'll scale it so that the feature selection isn't biased by the different scales of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b1be0f31",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 3.38 GiB for an array with shape (2907, 156192) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [24], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[0;32m      3\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[1;32m----> 4\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_imp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m scaler\u001b[38;5;241m.\u001b[39mtransform(X_train_imp)\n\u001b[0;32m      6\u001b[0m scaler\u001b[38;5;241m.\u001b[39mtransform(X_test_imp)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:806\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    805\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 806\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:841\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    809\u001b[0m \u001b[38;5;124;03m\"\"\"Online computation of mean and std on X for later scaling.\u001b[39;00m\n\u001b[0;32m    810\u001b[0m \n\u001b[0;32m    811\u001b[0m \u001b[38;5;124;03mAll of X is processed as a single batch. This is intended for cases\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    838\u001b[0m \u001b[38;5;124;03m    Fitted scaler.\u001b[39;00m\n\u001b[0;32m    839\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    840\u001b[0m first_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 841\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    845\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    847\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    848\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    849\u001b[0m n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    851\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py:566\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    564\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation should be done on X, y or both.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    565\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 566\u001b[0m     X \u001b[38;5;241m=\u001b[39m check_array(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    567\u001b[0m     out \u001b[38;5;241m=\u001b[39m X\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:746\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    744\u001b[0m         array \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mastype(dtype, casting\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsafe\u001b[39m\u001b[38;5;124m\"\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    745\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 746\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    749\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m    750\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:2064\u001b[0m, in \u001b[0;36mNDFrame.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   2063\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype: npt\u001b[38;5;241m.\u001b[39mDTypeLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m-> 2064\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:918\u001b[0m, in \u001b[0;36mDataFrame._values\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    916\u001b[0m blocks \u001b[38;5;241m=\u001b[39m mgr\u001b[38;5;241m.\u001b[39mblocks\n\u001b[0;32m    917\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(blocks) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\n\u001b[0;32m    920\u001b[0m arr \u001b[38;5;241m=\u001b[39m blocks[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m    921\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    922\u001b[0m     \u001b[38;5;66;03m# non-2D ExtensionArray\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:10892\u001b[0m, in \u001b[0;36mDataFrame.values\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m  10819\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m  10820\u001b[0m \u001b[38;5;124;03mReturn a Numpy representation of the DataFrame.\u001b[39;00m\n\u001b[0;32m  10821\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10889\u001b[0m \u001b[38;5;124;03m       ['monkey', nan, None]], dtype=object)\u001b[39;00m\n\u001b[0;32m  10890\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m  10891\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consolidate_inplace()\n\u001b[1;32m> 10892\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py:1599\u001b[0m, in \u001b[0;36mBlockManager.as_array\u001b[1;34m(self, dtype, copy, na_value)\u001b[0m\n\u001b[0;32m   1597\u001b[0m             arr \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1598\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1599\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interleave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1600\u001b[0m     \u001b[38;5;66;03m# The underlying data was copied within _interleave\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py:1638\u001b[0m, in \u001b[0;36mBlockManager._interleave\u001b[1;34m(self, dtype, na_value)\u001b[0m\n\u001b[0;32m   1635\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_dtype_equal(dtype, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1636\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1638\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1640\u001b[0m itemmask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m   1642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m na_value \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[0;32m   1643\u001b[0m     \u001b[38;5;66;03m# much more performant than using to_numpy below\u001b[39;00m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 3.38 GiB for an array with shape (2907, 156192) and data type float64"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_imp)\n",
    "scaler.transform(X_train_imp)\n",
    "scaler.transform(X_test_imp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b698f4",
   "metadata": {},
   "source": [
    "Now that I have dummy columns for all categorical values with 30 or more entries, I'm going to trim down the dataframe to 400 features based on f-regression for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598e5820",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = SelectKBest(f_regression, k=400)\n",
    "transformed = selector.fit_transform(X_train_imp, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f29445",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = selector.get_support(indices=True)\n",
    "X_train_selected = X_train_imp.iloc[:,features]\n",
    "X_test_selected = X_test_imp.iloc[:,features]\n",
    "X_train_selected.shape, X_test_selected.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dfa48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_selected['price_per_kw']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4f6aa2",
   "metadata": {},
   "source": [
    "Great! Our data looks good to go. I'll export all four portions of data separately for the modeling portion of the project and they can each be read in separately to that notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e104f028",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_selected.to_csv('X_train.csv')\n",
    "X_test_selected.to_csv('X_test.csv')\n",
    "y_train.to_csv('y_train.csv')\n",
    "y_test.to_csv('y_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0b4e8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
