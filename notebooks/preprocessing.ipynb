{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beb035ae",
   "metadata": {},
   "source": [
    "# Getting the most solar power for your dollar\n",
    "## Preprocessing and feature engineering\n",
    "### Zachary Brown\n",
    "\n",
    "The data has been cleaned and preliminary analysis has identified some trends we should expect to see the eventual model pick up on. Now I'm going to preprocess the data so that any models I work with can use the data appropriately. This will include imputing missing data, feature engineering, scaling, and splitting the data into testing and training datasets.\n",
    "\n",
    "I'll start by loading the necessary packages and reading in the data from the exploratory data analysis portion of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7e8cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme('notebook')\n",
    "import scipy.stats\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e62ee15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())\n",
    "os.chdir(r\"..\\data\\processed\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa21dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('processed_data.csv', index_col=0, na_values = [-1, '-1'], low_memory=False)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703f66c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns.groupby(data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04983731",
   "metadata": {},
   "source": [
    "I'm going to convert zip code from 9 digit to 5, and then switch the data type from object to integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8d41f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['zip_code'] = data['zip_code'][0:5].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8327fb6a",
   "metadata": {},
   "source": [
    "I want to check each column for the percentage of values that are missing, and remove any features with more than 30% missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d2ef25",
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_missing = data.isnull().sum()/len(data)*100\n",
    "percent_missing.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fde421",
   "metadata": {},
   "source": [
    "The only feature that needs to be removed due to null values is 'date_of_battery_install'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860d0ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop(columns=['date_of_battery_install'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43abc67a",
   "metadata": {},
   "source": [
    "Next I want to browse the object columns and count how many unique values each has. If a feature has too many or only one unique value they won't help identify any trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b464e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in data.columns:\n",
    "    if data[col].dtypes == 'object':\n",
    "        print(col, ' : ', data[col].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dfe0d3",
   "metadata": {},
   "source": [
    "Based on these results it should be safe to remove system_id_1, as that has a unique value for almost every entry. I'll also drop customer_segment since earlier in the project I limited the dataset to only residential installations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d73acbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['system_id_1', 'customer_segment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d207fb7",
   "metadata": {},
   "source": [
    "Great! Now I need to encode these categorical features as I did with the states earlier. To do so I'll check the number of entries for each unique value for any feature with more than 30 unique values (anything below 30 I'll just dummy encode like I did with states). If certain values appear in more than 10% of the entries then I'll check to see if they correlate with price per KW when compared against all other values for that feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df935cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.get_dummies(data, columns=['technology_module_1', 'data_provider_1'])\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df4cb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['installation_date', 'zip_code', 'city', 'utility_service_territory', 'installer_name', 'module_manufacturer_1',\\\n",
    "        'module_model_1', 'inverter_manufacturer_1', 'inverter_model_1']\n",
    "for col in cols:\n",
    "    print(col, ':\\n', data[col].value_counts(normalize=True).loc[lambda x : x>0.1], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52315f0b",
   "metadata": {},
   "source": [
    "Based on these distributions I'm going to drop installation_date and installer_name since none of their values account for 10% or more of the entries in the dataset. For the values that do account for at least 10% of the data I'll perform t-tests comparing the price_per_kw for entries with that value vs the rest of the entries. If the p-value of the t-test is less than 0.01 then I'll create a dummy column for it. This includes treating missing data as its own value, since there could be a correlation there as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16c14aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['installation_date', 'installer_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a449083c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility service territory: Pacific Gas and Electric\n",
    "pge = data[data['utility_service_territory'] == 'Pacific Gas and Electric']\n",
    "not_pge = data[data['utility_service_territory'] != 'Pacific Gas and Electric']\n",
    "print(scipy.stats.ttest_ind(pge['price_per_kw'], not_pge['price_per_kw']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d3a145",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['territory_pacific_gas_and_electric'] = (data['utility_service_territory'] == 'Pacific Gas and Electric')*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab49b8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility service territory: Southern California Edison\n",
    "sce = data[data['utility_service_territory'] == 'Southern California Edison']\n",
    "not_sce = data[data['utility_service_territory'] != 'Southern California Edison']\n",
    "print(scipy.stats.ttest_ind(sce['price_per_kw'], not_sce['price_per_kw']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171c8bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility service territory: San Diego Gas and Electric\n",
    "sdge = data[data['utility_service_territory'] == 'San Diego Gas and Electric']\n",
    "not_sdge = data[data['utility_service_territory'] != 'San Diego Gas and Electric']\n",
    "print(scipy.stats.ttest_ind(sdge['price_per_kw'], not_sdge['price_per_kw']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca46576",
   "metadata": {},
   "source": [
    "At this point I'm going to make a copy of the dataframe as a bookmark preceding any data imputation or data loss. My next step will impute missing data as an 'other' category, and after initial modeling I may want to jump back to before this step to rework how I handle those missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb25da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_imputation = data.copy()\n",
    "no_imputation.to_csv('pre-imputation preprocessing data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39571dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['territory_san_diego_gas_and_electric'] = (data['utility_service_territory'] == 'San Diego Gas and Electric')*1\n",
    "data['utility_service_territory_other'] = (~data['utility_service_territory'].isin(['Pacific Gas and Electric',\\\n",
    "                                                                                     'San Diego Gas and Electric']))*1\n",
    "data=data.drop(columns=['utility_service_territory'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c5049f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module manufacturer 1: Hanwha Q CELLS\n",
    "hqc = data[data['module_manufacturer_1'] == 'Hanwha Q CELLS']\n",
    "not_hqc = data[data['module_manufacturer_1'] != 'Hanwha Q CELLS']\n",
    "print(scipy.stats.ttest_ind(hqc['price_per_kw'], not_hqc['price_per_kw']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e285a076",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['hanwha_q_cells'] = (data['module_manufacturer_1'] == 'Hanwha Q CELLS')*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f0f5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module manufacturer 1: SunPower\n",
    "sp = data[data['module_manufacturer_1'] == 'SunPower']\n",
    "not_sp = data[data['module_manufacturer_1'] != 'SunPower']\n",
    "print(scipy.stats.ttest_ind(sp['price_per_kw'], not_sp['price_per_kw']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cec63ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sunpower'] = (data['module_manufacturer_1'] == 'SunPower')*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50480b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module manufacturer 1: Missing\n",
    "missing = data[data['module_manufacturer_1'].isna()]\n",
    "not_missing = data[~data['module_manufacturer_1'].isna()]\n",
    "print(scipy.stats.ttest_ind(missing['price_per_kw'], not_missing['price_per_kw']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12adcd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['module_manufacturer_1_missing'] = (data['module_manufacturer_1'].isna())*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6c5740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module manufacturer 1: LG Electronics\n",
    "lg = data[data['module_manufacturer_1'] == 'LG Electronics']\n",
    "not_lg = data[data['module_manufacturer_1'] != 'LG Electronics']\n",
    "print(scipy.stats.ttest_ind(lg['price_per_kw'], not_lg['price_per_kw']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db608c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['lg_electronics'] = (data['module_manufacturer_1'] == 'LG Electronics')*1\n",
    "data['module_manufacturer_1_other'] = (~data['module_manufacturer_1'].isin(['Hanwha Q CELLS',\\\n",
    "                                                                                     'SunPower', 'LG Electronics', np.NaN]))*1\n",
    "data=data.drop(columns=['module_manufacturer_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da894e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module model 1: missing values\n",
    "missing = data[data['module_model_1'].isna()]\n",
    "not_missing = data[~data['module_model_1'].isna()]\n",
    "print(scipy.stats.ttest_ind(missing['price_per_kw'], not_missing['price_per_kw']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124daca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['module_model_1_missing'] = (data['module_model_1'].isna())*1\n",
    "data['module_model_1_not_missing'] = (~data['module_model_1'].isna())*1\n",
    "data = data.drop(columns=['module_model_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb50385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverter manufacturer 1: SolarEdge Technologies\n",
    "se = data[data['inverter_manufacturer_1'] == 'SolarEdge Technologies']\n",
    "not_se = data[data['inverter_manufacturer_1'] != 'SolarEdge Technologies']\n",
    "print(scipy.stats.ttest_ind(se['price_per_kw'], not_se['price_per_kw']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823434eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['solaredge_technologies'] = (data['inverter_manufacturer_1'] == 'SolarEdge Technologies')*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6b546b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverter manufacturer 1: Enphase Energy\n",
    "ee = data[data['inverter_manufacturer_1'] == 'Enphase Energy']\n",
    "not_ee = data[data['inverter_manufacturer_1'] != 'Enphase Energy']\n",
    "print(scipy.stats.ttest_ind(ee['price_per_kw'], not_ee['price_per_kw']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8ff105",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['enphase_energy'] = (data['inverter_manufacturer_1'] == 'Enphase Energy')*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c8767b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverter manufacturer 1: Missing\n",
    "missing = data[data['inverter_manufacturer_1'].isna()]\n",
    "not_missing = data[~data['inverter_manufacturer_1'].isna()]\n",
    "print(scipy.stats.ttest_ind(missing['price_per_kw'], not_missing['price_per_kw']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2eac50",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['inverter_manufacturer_1_missing'] = (data['inverter_manufacturer_1'].isna())*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1968a4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverter manufacturer 1: SunPower\n",
    "sp = data[data['inverter_manufacturer_1'] == 'SunPower']\n",
    "not_sp = data[data['inverter_manufacturer_1'] != 'SunPower']\n",
    "print(scipy.stats.ttest_ind(sp['price_per_kw'], not_sp['price_per_kw']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688b07a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sunpower'] = (data['inverter_manufacturer_1'] == 'SunPower')*1\n",
    "data['inverter_manufacturer_1_other'] = (~data['inverter_manufacturer_1'].isin(['SolarEdge Technologies',\\\n",
    "                                                                                     'Enphase Energy', 'SunPower', np.nan]))*1\n",
    "data=data.drop(columns=['inverter_manufacturer_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a2210b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverter model 1: Missing\n",
    "missing = data[data['inverter_model_1'].isna()]\n",
    "not_missing = data[~data['inverter_model_1'].isna()]\n",
    "print(scipy.stats.ttest_ind(missing['price_per_kw'], not_missing['price_per_kw']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2335b11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['inverter_model_1_missing'] = (data['inverter_model_1'].isna())*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926a434e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverter model 1: IQ7-60-2-US [240V]\n",
    "iq7 = data[data['inverter_model_1'] == 'IQ7-60-2-US [240V]']\n",
    "not_iq7 = data[data['inverter_model_1'] != 'IQ7-60-2-US [240V]']\n",
    "print(scipy.stats.ttest_ind(iq7['price_per_kw'], not_iq7['price_per_kw']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a798118b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['iq7'] = (data['inverter_model_1'] == 'IQ7-60-2-US [240V]')*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7db4e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverter model 1: SE3800H-US [240V]\n",
    "se3 = data[data['inverter_model_1'] == 'SE3800H-US [240V]']\n",
    "not_se3 = data[data['inverter_model_1'] != 'SE3800H-US [240V]']\n",
    "print(scipy.stats.ttest_ind(se3['price_per_kw'], not_se3['price_per_kw']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2a2609",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['se3'] = (data['inverter_model_1'] == 'SE3800H-US [240V]')*1\n",
    "data['inverter_model_1_other'] = (~data['inverter_model_1'].isin(['IQ7-60-2-US [240V]',\\\n",
    "                                                                                     'SE3800H-US [240V]', np.nan]))*1\n",
    "data=data.drop(columns=['inverter_model_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f0ce15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.columns.groupby(data.dtypes))\n",
    "\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2be7858",
   "metadata": {},
   "source": [
    "Great! All of the non-numeric features have been converted into dummy features or dropped. \n",
    "\n",
    "It's important to note that for utility service territory I've imputed the missing values as 'other'. This may need to be adjusted later on as I work through modeling.\n",
    "\n",
    "Now the rest of the data imputation and scaling needs to be performed on the training dataset, then applied to the test dataset, so now that all of the desired features have been created I'll split the data into test and train sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0dcb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data.drop(columns='price_per_kw'), data['price_per_kw'], test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6259dce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edd44db",
   "metadata": {},
   "source": [
    "Now I need to check the numeric columns to see if they have -1 for missing data, then decide how best to replace those for the modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351b64e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in X_train.columns:\n",
    "    if X_train[col].isna().sum() != 0:\n",
    "        print(col, ' : ', (X_train[col].isna().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f37b5fa",
   "metadata": {},
   "source": [
    "First I'll work through categorical numerical columns that only have 0/1 categories to check the distributions and determine how best to impute the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b899883",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['self_installed', 'tracking', 'ground_mounted', 'third_party_owned', 'bipv_module_1', 'bifacial_module_1',\\\n",
    "      'additional_inverters', 'additional_modules', 'dc_optimizer', 'micro_inverter_1', 'built_in_meter_inverter_1',\\\n",
    "      'solar_storage_hybrid_inverter_1']\n",
    "X_train[cols].apply(func = pd.Series.value_counts, args=('normalize', True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c0411b",
   "metadata": {},
   "source": [
    "Many of these columns are heavily skewed, so I'll impute the missing values using the mode and assign the missing values to the more heavily favored response. When I begin modeling I'll compare the model using the imputed results vs removing the columns completely to determine whether I need to reasses how to impute these missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3899af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[cols] = X_train[cols].fillna(0)\n",
    "X_test[cols] = X_test[cols].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b3cd0f",
   "metadata": {},
   "source": [
    "Now the more complicated categorical features: zip code and city. For zip code I think it makes sense to impute them with the most common value for the state in which that sample lies. The city should probably be the most common city within the zip code of the sample. I'll start with imputing zip code first, then filter down to city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97be7ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uszipcode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531e0102",
   "metadata": {},
   "source": [
    "Now that the categorical numerical features have been taken care of I'll recheck what continuous numerical variables are left to clean up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe2af03",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=[]\n",
    "for col in X_train.columns:\n",
    "    if X_train[col].isna().sum() != 0:\n",
    "        cols.append(col)\n",
    "        print(col, ' : ', (round(X_train[col].isna().sum()/len(X_train[col]), 2) * 100), '% missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b115fce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in X_train[cols]:\n",
    "    bins=int(round(np.sqrt(len(data[col].unique()))))\n",
    "    sns.histplot(data=X_train[col], bins=bins)\n",
    "    plt.title(col)\n",
    "    plt.ylabel('Installations')\n",
    "    plt.xlabel(col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7c18bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['azimuth_1'].value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d8103e",
   "metadata": {},
   "source": [
    "Right now my goal is to get a quick first look at some models so that I can determine which features are important and then refine the imputation of any missing data for those features if needed. To that end, I'm going to impute these missing values as the mode for azimuth_1 since it's an angle and there is an obvious preference for one specific angle, and the median for the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954c3af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "mode_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "X_train['azimuth_1'] = mode_imputer.fit_transform(X_train['azimuth_1'].values.reshape(-1,1))\n",
    "X_test['azimuth_1'] = mode_imputer.fit_transform(X_test['azimuth_1'].values.reshape(-1,1))\n",
    "\n",
    "\n",
    "cols.remove('azimuth_1')\n",
    "\n",
    "median_imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "for col in X_train[cols]:\n",
    "    X_train[col] = median_imputer.fit_transform(X_train[col].values.reshape(-1,1))\n",
    "    \n",
    "for col in X_test[cols]:\n",
    "    X_test[col] = median_imputer.fit_transform(X_test[col].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e403e2",
   "metadata": {},
   "source": [
    "Now I'll do one last check of the entire dataframe to confirm that there are no more null values or -1s present, then we should be ready to start modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa492ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total null values in dataframe\n",
    "print(X_train.isna().sum().sum())\n",
    "print(X_test.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4212cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total -1 values in dataframe\n",
    "print((X_train.values == -1).sum())\n",
    "print((X_test.values == -1).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcab8c5",
   "metadata": {},
   "source": [
    "Great! Our data looks good to go. I'll export all four portions of data separately for the modeling portion of the project and they can each be read in separately to that notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9b09ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv('X_train.csv')\n",
    "X_test.to_csv('X_test.csv')\n",
    "y_train.to_csv('y_train.csv')\n",
    "y_test.to_csv('y_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646c4e18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
